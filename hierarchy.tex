\title{An argument against hierarchy in software projects}
\author{
        James Baldwin-Brown \\
}
\date{\today}

\documentclass[12pt]{article}
\usepackage{tabu}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{csquotes}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}

\begin{document}
\maketitle

% \begin{abstract}
% \end{abstract}

\section{Introduction}

I've been learning a new programming language, Go. I've also been reading some
classic literature about Unix and the open source / hacker philosophy. There's
a lot that both of these sources (\textit{sources}! Okay, I'll show myself
out.) have to say, but one theme that I keep seeing repeated is that of
hierarchy, and the problems inherent in it. Typical programming, especially in
object-oriented languages, is highly hierarchical. A lot of a programmer's time
is spent thinking things like, ``Should A be a subclass of B, or should B be a
subclass of A and C?'' I'm going to argue here that this style of structuring a
project sounds good in theory, but ultimately leads to inflexible software that
is difficult to re-use. Instead, I argue for a style of programming that
minimizes hierarchy, giving the maximum flexibility and power to the eventual
user of software. This sometimes means writing more code up front, but in the
vast majority of cases, this extra code is straightforward boilerplate that is
both quick to write and light on bugs.

I'm going to argue against hierarchy at two levels: the level of data
structures within a code library, and the level of libraries within a project.
At the level of individual data structures, rather than building a hierarchy of
data classes, I will argue for the advantages of Go-style interfaces, which
have effectively only two levels: structures, which hold data, and interfaces,
which specify patterns for manipulating structures.  At the level of libraries
in a project, I have noticed a trend toward more hierarchical project
structures over time due largely to the use of interpreted languages.  I am
going to argue against the interpreted language paradigm of extending and
embedding interpreters, in favor of the flexible, hierarchy-free method of
directly linking compiled object files.

\section{The problem with hierarchy}

\section{The small scale: interfaces over classes}

I'm going to pick on object oriented programming (OOP) for a minute here. If you open a programming textbook, it will tell you that the canonical four
pillars of OOP are:

\begin{enumerate}
    \item Encapsulation: binding together data and functions into objects to prevent misuse and reduce global complexity.
    \item Composition: Objects can hold other objects.
    \item Inheritance: `Subclasses' that are (confusingly) strict supersets of another class `inherit' the methods of the `superclass', allowing for implicit code re-use.
    \item Polymorphism: The ability for a function to call different code depending on the type of its argument(s).
\end{enumerate}

What the proponents of OOP told people in its heyday was that it was the
best, or maybe only, way to achieve these goals. Although we usually associate
OOP with languages designed to facilitate the use of OOP practices, all of the
major OOP paradigms can be implemented in non-OOP languages such as C. Of the
four items above, three are (mostly) unambiguously useful: encapsulation keeps
the complexity of a project low, composition lets you build big objects out of
small, modular pieces, and polymorphism allows a language to operate at a
higher level of abstraction.  Inheritance is the odd one out. The point of
inheritance is to let you re-use code, but one of the most common and confusing
types of errors comes from multiple inheritance name conflicts, where you
inherit methods from two different classes, each with the same method name but
different code. A quickie solution for this is the removal of multiple
inheritance, but then code re-use is destroyed -- a given class can only
inherit code from one other class.  This is where the hierarchy becomes a
problem: classes in a hierarchy can only inherit from other classes higher in
the hierarchy.

There are lots of solutions to this problem, but Go's solution is an especially
interesting one.  In Go, there are only structs (classes that just contain
data), which are at the bottom of the hierarchy, and interfaces (equivalent to
abstract classes in the OOP world -- just a collection of function pointers).
There is no inheritance from interface to structs, and interfaces never hold
any data. Because interfaces cannot inherit from other interfaces, this is a
flat hierarchy with only two levels: structs at the bottom, and interfaces at
the top, defining how structs can interact.  The advantage of the interface
worldview that Go promotes is that any struct can fulfill any interface, and
functions can take interfaces as arguments. So, to write code that is easily
reusable, write code that works on interfaces, and then you can always
implement that interface on a struct as needed. Let's see an example. Go is
built around interfaces, but let's implement one in C so we can see that it's a
language-agnostic way of looking at data and code.

\begin{lstlisting}[language=C]
#include <stdio.h>

/* The template for the virtual function table for printables */
typedef struct printable_vtable {
    void (*print) (void *);
} printable_vtable;

/* Define the printable interface. Note that all interfaces have the same structure:
a void pointer to the underlying concrete type, and a pointer to the concrete type's
implementation of the interface vtable */

typedef struct printable {
    void *val;
    printable_vtable *vt;
} printable;

/* Implement each of the interface's vtable functions as a
polymorphic function for ease of use */

void printable_print(printable p) {
    p.vt->print(p.val);
}

/* Below, we are going to implement a 'printable' interface for 'int' objects */

/* Define how to print ints */

void print_int(void *vi) {
    int *i = vi;
    printf("%d", *i);
}

/* Define a global printable vtable for ints */

printable_vtable int_printable_vtable = {
    .print = print_int
};

/* A function to wrap an int in the printable interface: */

printable int2printable(int *i) {
    return (printable) {
        .val = i,
        .vt = &int_printable_vtable
    };
}
\end{lstlisting}

This makes an interface for data that can be printed. For any data type,
you can implement the functions that this interface needs. Once you've done that,
any function that uses this interface as an argument can be used indirectly on
your data. Here's an example of running `printable\_print' on an integer:

\begin{lstlisting}[language=C]
int main() {
    int i=25;
    printable_print(int2printable(&i));
    /* This will print '25' */
    
    return 0;
}
\end{lstlisting}

Implementing the interface for new types just means implementing the functions for the interface, then wrapping in a vtable:

\begin{lstlisting}[language=C]
printable_vtable double_printable_vtable = {
    .print = print_double
};

void print_double(void *vd) {
    double *d = vd;
    printf("%lf", *d);
}

printable double2printable(double *d) {
    return (printable) {
        .val = d,
        .vt = &double_printable_vtable
    };
}
\end{lstlisting}

Here's where things get good. You can easily write new functions that use the printable interface:

\begin{lstlisting}[language=C]
void printable_verbose_print(printable p, char *varname) {
    printf("The value of %s is: ", varname);
    printable_print(p);
    putchar('\n');
}
\end{lstlisting}

These new functions will instantly work with
any types that fulfill the printable interface:

\begin{lstlisting}[language=C]
int main() {
    int i=25;
    double d=3708.8;
    printable_print(int2printable(&i));
    puts("");
    /* '25' */
    printable_print(double2printable(&d));
    puts("");
    /* '3708.800000' */
    printable_verbose_print(int2printable(&i), "i");
    /* 'The value of i is: 25' */
    printable_verbose_print(double2printable(&d), "d");
    /* 'The value of d is: 3708.800000' */
    
    return 0;
}
\end{lstlisting}

This is the advantage of a hierarchy-free abstraction: the number of
permutations of (interface function, concrete type) is proportional to 
the number of interface functions times the number of concrete types,
but the amount of code (and the complexity of your project) is proportional
to the number of functions \textit{plus} the number of types.

The downside of this paradigm is extra boilerplate in the implementation of interfaces.
A common situation is pointer or getter/setter functions. Say you're writing a video game,
and you have a set of characters, some of which are capable of fighting. If you have a `fighter'
interface that looks like this:

\begin{lstlisting}[language=C]

typedef struct fighter_vtable {
    double *(*health) (void *);
    double *(*power) (void *);
    double *(*defense) (void *);
}

\end{lstlisting}

For each of the critters that implements the `fighter' interface, you're going to have to
write boring functions to get pointers to their important stats:

\begin{lstlisting}[language=C]
double *critter1_health(void *v) {
    critter1 *c = v;
    return &c->health;
}
double *critter2_health(void *v) {
    critter2 *c = v;
    return &c->health;
}
double *critter3_health(void *v) {
    critter3 *c = v;
    return &c->health;
}
/* ... */
\end{lstlisting}

The important thing about this boring code is that it's \textit{boring}. It's
highly repetitive, straightforward, and difficult to mess up. Everything works
exactly as you think it will, and there are no surprises of the type you get
with complex multiple inheritance. The limiting factor in writing programs tends
not to be how many lines of boilerplate you have to write, but how many complicated
interactions there are between pieces of code. Well defined interfaces allow for
code reuse while keeping the set of possible weird interactions low.

\subsection{Other programming paradigms}

If you already dislike object oriented programming, you may be thinking,
``Yeah, he's right -- inheritance was a terrible idea! I'm glad that I use
functional programming, where these kinds of problems can't happen.'' Or
similar. The truth is that most programming paradigms have mechanisms
comparable to those in OOP. For example, closures provide the same technical
capabilities as objects. Product types such as tuples are equivalent to structs,
and sum types have many of the same issues as super- and sub-classes.
That said, the same philosophy used in the examples above can be applied regardless
of programming paradigm:

\begin{enumerate}
    \item Separate your data from code interfaces.
    \item Write code that operates on interfaces.
    \item Keep the hierarchy down to two levels: the data level and the interface level.
\end{enumerate}

\section{The large scale: linking over embedding and extending}

Let's talk about big projects. A big project almost never consists entirely of
your own code. In fact, in most cases, the majority of the code in your project
will originate not from you, but from libraries generated by other people that
had different goals than you in mind. The library-centric approach to
programming that we often see nowadays owes a great deal to the Unix tradition.
This is so true that even opponents of Unix could see its merits. Richard P.
Gabriel, one of the big proponents of the Lisp machine and a perennial Unix
hater, pointed out Unix's strengths (what he calls ``worse-is-better'') in
``Lisp: Good News, Bad News, How to Win Big'':

\begin{displayquote}
In the worse-is-better world, integration is linking your .o files together,
freely intercalling functions, and using the same basic data representations.
You don’t have a foreign loader, you don’t coerce types across function-call
boundaries, you don’t make one language dominant, and you don’t make the woes
of your implementation technology impact the entire system.

The very best Lisp foreign functionality is simply a joke when faced with the
above reality. Every item on the list can be addressed in a Lisp
implementation. This is just not the way Lisp implementations have been done in
the right thing world.
\end{displayquote}

What he notices here is the advantage of a lack of hierarchy in Unix. Any
library, once bound to the C API, can be called from any language and run as
native code. That means all of your C, Fortran, C++, Go, Rust, Pascal, and (God help
you) COBOL libraries can be linked together and run using any language.  You
don't even need to know what the implementation language for a given library
is: as long as you have a .o, .a, .dll, or .sa file to link, things will just work.

Let's contrast that with the Lisp world that Gabriel was describing above.
Lisp is largely a relic now (thanks in part to the very problem Gabriel
describes), but its philosophy about interoperation with libraries and
languages lives on in interpreted languages such as Python, Java, Ruby, Lua, R,
and shell. Indeed, the FFI (foreign function interface) libraries used by most
of these for calling code from other languages are named after the original
Lisp FFI. There is a natural hierarchy separating compiled languages, which run
directly on the machine, from interpreted languages, which run on a simulated
machine that we call an interpreter (or virtual machine, or whatever else). A
consequence of this is that all compiled languages sit together at the same
level of the hierarchy, and they can call each other's functions as long as
they conform to a consistent programming API and ABI, such as the C API.  This
non-hierarchical structure is part of the reason that so much Fortran code is
still around: not a lot of people write Fortran any more outside of the high
performance computing world, but there are a ton of Fortran libraries still in
use, such as [], [], and []. The libraries conform to the C API, which means
they can be called from any other programming language. This means the Fortran
libraries can outlive Fortran culture: as long as working Fortran compilers
exist and people can do minimal security maintenance of old Fortran code, this
code never needs to be rewritten.

Interpreted languages, on the other hand, each consider themselves to be the
top of the hierarchy, with all other languages below them. Let's take Python as
an example.  It's very easy to write a wrapper for C code in Python
(``extending'' Python with C). There are several different systems to assist
this process, including the ctypes library, SWIG, and cython. Because Python is
written in C, you can even write your own Python package in pure C,
hand-writing your own wrappers for your C code and then loading the compiled
object file as a ``native'' Python package.

Unfortunately, the obverse is quite a challenge: calling Python from another
language, such as C, requires you to start an instance of the Python
interpreter, keep it live for as long as you need any Python objects or
functions, and then close it. This is what is meant by ``embedding'' Python in
a C executable. Actually doing this is pretty annoying, and it dramatically
increases the barrier to using Python code outside of Python.  There's no way
to quickly link a single Python function, call it once, and forget it. Lua is
perhaps the easiest language to embed in an executable from another language,
but it accomplishes this by having very few features and a tiny standard
library. It also still requires wrappers to get objects onto and off of the Lua
stack, making it still more troublesome than linking .o files.

This gets worse in the case where one interpreted language needs to call
functions in another. Let's say you are writing a statistics package in Python,
and you need to perform a Fisher's exact test (FET) in a situation where more
than two test outcomes are possible. Python can do a two-case FET fine, but
there simply isn't support for three or more cases. You look around and find
that R's FET implementation can handle an arbitrary number of cases. So now you
need to get R's FET function to run in Python. There are several ways to
accomplish this, none of them easy. You can:

\begin{enumerate}
    \item Try to start up an R interpreter in Python using the (pretty cool) rpy2 package.
    \item Write a shell script that runs your Python code, saves your data in text files, uses those as input for the R code, saves the R output in text files, then runs some more Python code on the R output. This one can be made easier using pipes, but R doesn't play nice with pipes.
    \item Write a Python wrapper that uses Popen or similar to open R, and pipe your data from Python to R in a structured format. I did this once to call R's ggplot package from Python by piping data in json format, but the differences in Python's NaN, json's nil, and R's NA caused lots of bugs.
    \item Just give up and re-write your code in R.
\end{enumerate}

If you have experience with either of these languages, the gears in your head
are probably already turning, thinking of solutions to this problem I haven't
listed here. There may well be an easier way to solve this problem, but that's
not the point.  The point is that, if R's FET package had been written in a
compiled language, the problem wouldn't exist in the first place. Even if it
was written in ancient Fortran, as long as it used the C API, \textit{any}
compiled language could link it without knowing \textit{what} it was written
in. It wouldn't matter! And it shouldn't matter. One thing that is certainly
true in the computing world is that things will keep changing. The programming
languages that are wildly popular now may not exist at all in a few years. All
of the code written in an interpreted language is headed for the dustbin of
history. Consider Perl, perhaps the most widely used interpreted language of
all time.  After its heyday in the '80s and '90s, it was largely replaced with
Python and other easier-to-read languages. There are tons of very good perl
libraries out there like [] and [], but the only programs that use those
libraries are perl programs, and there are fewer of those in use every year.
Compare that with Fortran, which was written in the '50s and whose libraries
still underpin new programs written in all kinds of languages. There is a real
cost to writing in an interpreted language: all of your code is temporary,
and will eventually need to be rewritten from scratch to be callable in the next
popular interpreter. If you want your code to stay alive, it needs to be compiled.

\section{C as a lingua franca, and other hidden hierarchies}

You might be reading this and thinking, ``He's saying compiled code doesn't
have hierarchy, but it all has to conform to the C API to interoperate, and
that means C is really sitting at the top of the hierarchy. He's really just
another C partisan who's trying to sneak his favorite language to the top of
the heap.'' I hear you, and I can't deny that I'm a big fan of languages like
C. I'm as biased as anyone, but I think my arguments above stand anyway. One of
the reasons I've used languages like Fortran in my examples is to show that,
even if C's API became standard through accidents of history, the fact that
compiled languages have a single standardized API is a huge advantage. It
wouldn't matter if a different language had been the standard the way that
Pascal almost was on the original Mac -- any standard, even a bad one, allows
for the collapse of hierarchy into a flat domain in which all languages
interoperate cleanly.

Another thing that should be noted is that compiled code is very often called
from an interpreted language. In the Unix world, almost all programs
historically were started by the shell, which is an interpreter. I can't
realistically say to never use an interpreted language, because that would
ultimately mean running all code in batch mode. No one wants to go back to
non-interactive computers. What I am saying is, don't write important code in
interpreted languages. We should re-think what interpreted languages are for,
and think of them more like simple shells for running compiled code. Anything
that might be worth using again, even if you don't see exactly how it will be
used, should be written in a compiled language. Otherwise, it will all get
thrown out in the long run.


\section{Go forth, and create reusable code}

% \lstinline{ed} is still as usable today as
% it was in 1969, and using \lstinline{ed} for a few days or weeks will let

% \begin{table}
%     \begin{center}
%         \begin{tabu}to 0.8\linewidth {XXX}
%             Card Name & Numerical value & Alphabetic mapping\\
%             \hline\\
%             King & 0 & D\\
%             Ace & 1 & A\\
%             Deuce & 2 & K\\
%             3 & 3 & H\\
%             4 & 4 & E\\
%             5 & 5 & B\\
%             6 & 6 & L\\
%             7 & 7 & I\\
%             8 & 8 & F\\
%             9 & 9 & C\\
%             10 & 10 & M\\
%             Jack & 11 & J\\
%             Queen & 12 & G\\
%         \end{tabu}
%         \caption{Mapping to a new address space.}
%         \label{table:map}
%     \end{center}
% \end{table}

\end{document}
